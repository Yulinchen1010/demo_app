from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import Response
from pydantic import BaseModel, Field
from typing import Optional, List, Dict
import sqlite3
import pandas as pd
import numpy as np
import os
from datetime import datetime, timedelta, timezone
import io
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from matplotlib import font_manager
import warnings
warnings.filterwarnings('ignore')

from fatigue_pipeline import (
    HORIZON_S,
    PipelineArtifacts,
    prepare_pipeline_from_csv,
    predict_high_fatigue_probability,
    predict_future_level,
)

# è¨­å®šä¸­æ–‡å­—é«”
def setup_chinese_font():
    """è¨­å®š matplotlib ä¸­æ–‡å­—é«”"""
    try:
        # å˜—è©¦ä½¿ç”¨ç³»çµ±ä¸­æ–‡å­—é«”
        fonts = ['Microsoft YaHei', 'SimHei', 'Arial Unicode MS', 'STHeiti', 'PingFang TC', 'Noto Sans CJK TC']
        for font in fonts:
            if any(font.lower() in f.name.lower() for f in font_manager.fontManager.ttflist):
                plt.rcParams['font.sans-serif'] = [font]
                plt.rcParams['axes.unicode_minus'] = False
                print(f"âœ… ä½¿ç”¨å­—é«”: {font}")
                return
        # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨ DejaVu Sansï¼ˆé¡¯ç¤ºè‹±æ–‡ï¼‰
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
        print("âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—é«”ï¼Œä½¿ç”¨è‹±æ–‡é¡¯ç¤º")
    except:
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
        print("âš ï¸ å­—é«”è¨­å®šå¤±æ•—ï¼Œä½¿ç”¨é è¨­å­—é«”")

try:
    setup_chinese_font()
except Exception:
    pass

# ==================== åˆå§‹è¨­å®š ====================
DB_PATH = os.getenv("DB_PATH", "fatigue_data.db")

# è¨­å®šæ™‚å€ï¼šå°ç£æ™‚é–“ (UTC+8)
TZ_TAIWAN = timezone(timedelta(hours=8))

def get_taiwan_time():
    """å–å¾—å°ç£ç•¶å‰æ™‚é–“"""
    return datetime.now(TZ_TAIWAN)

app = FastAPI(title="ç–²å‹é æ¸¬ç³»çµ±", version="5.0")

# CORS è¨­å®šï¼ˆå…è¨± APP é€£æ¥ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== è³‡æ–™æ¨¡å‹ ====================
class SensorData(BaseModel):
    worker_id: str
    percent_mvc: float = Field(ge=0, le=100)
    timestamp: Optional[str] = None

class BatchUpload(BaseModel):
    data: List[SensorData]
    
class RulaData(BaseModel):
    worker_id: str
    rula: dict
    timestamp: Optional[str] = None

# ========== RULA ç°¡åŒ–è¼¸å…¥æ¨¡å‹ ==========
## RULA scoring is handled on the client app. No server-side model required.

# ==================== è³‡æ–™åº«åˆå§‹åŒ– ====================
def init_db():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("""
        CREATE TABLE IF NOT EXISTS sensor_data (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            worker_id TEXT NOT NULL,
            timestamp TEXT NOT NULL,
            percent_mvc REAL NOT NULL
        )
    """)
    c.execute("CREATE INDEX IF NOT EXISTS idx_worker_timestamp ON sensor_data(worker_id, timestamp)")
    c.execute("""
        CREATE TABLE IF NOT EXISTS rula_data (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            worker_id TEXT NOT NULL,
            timestamp TEXT NOT NULL,
            score INTEGER NOT NULL,
            risk_label TEXT
        )
    """)
    c.execute("CREATE INDEX IF NOT EXISTS idx_rula_worker_timestamp ON rula_data(worker_id, timestamp)")
    conn.commit()
    conn.close()

init_db()

# ==================== è¼‰å…¥ç–²å‹æ¨¡æ“¬è³‡æ–™èˆ‡æ¨¡å‹ ====================
SIM_CSV_PATH = os.getenv(
    "SIM_CSV_PATH",
    os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "fatigue_simulated_with_recovery.csv")),
)

PIPELINE_ARTIFACTS: Optional[PipelineArtifacts]
PIPELINE_FEATURES_VALID: pd.DataFrame
PIPELINE_FEATURE_COLUMNS: List[str]
PIPELINE_REPORTS: Dict[str, str]

try:
    PIPELINE_ARTIFACTS = prepare_pipeline_from_csv(SIM_CSV_PATH)
    PIPELINE_FEATURES_VALID = PIPELINE_ARTIFACTS.features.loc[
        PIPELINE_ARTIFACTS.valid_feature_mask
    ]
    PIPELINE_FEATURE_COLUMNS = PIPELINE_ARTIFACTS.feature_columns
    PIPELINE_REPORTS = PIPELINE_ARTIFACTS.classification_reports
    print(f"âœ… å·²è¼‰å…¥ç–²å‹æ¨¡æ“¬è³‡æ–™: {SIM_CSV_PATH}")
except Exception as exc:
    PIPELINE_ARTIFACTS = None
    PIPELINE_FEATURES_VALID = pd.DataFrame()
    PIPELINE_FEATURE_COLUMNS = []
    PIPELINE_REPORTS = {}
    print(f"âš ï¸ ç„¡æ³•è¼‰å…¥ç–²å‹æ¨¡æ“¬è³‡æ–™: {exc}")

# ==================== è¼”åŠ©å‡½æ•¸ ====================
def get_worker_data(worker_id: str) -> pd.DataFrame:
    conn = sqlite3.connect(DB_PATH)
    df = pd.read_sql_query(
        "SELECT * FROM sensor_data WHERE worker_id = ? ORDER BY timestamp DESC LIMIT 1000",
        conn, params=(worker_id,)
    )
    conn.close()
    if not df.empty:
        df['timestamp'] = pd.to_datetime(df['timestamp'])
    return df

# RULA simplified scoring removed: computed client-side in Flutter app.


def list_pipeline_sessions() -> List[str]:
    if PIPELINE_ARTIFACTS is None:
        return []
    return sorted(PIPELINE_ARTIFACTS.processed["session_id"].unique())


def get_pipeline_session(session_id: str) -> pd.DataFrame:
    if PIPELINE_ARTIFACTS is None:
        return pd.DataFrame()
    df = PIPELINE_ARTIFACTS.processed
    return df[df["session_id"] == session_id].sort_values("timestamp")


def get_pipeline_feature_row(session_id: str) -> Optional[pd.Series]:
    if PIPELINE_ARTIFACTS is None or PIPELINE_FEATURES_VALID.empty:
        return None
    df_feat = PIPELINE_FEATURES_VALID
    session_feat = df_feat[df_feat["session_id"] == session_id]
    if session_feat.empty:
        return None
    session_feat = session_feat.sort_values("timestamp")
    return session_feat.iloc[-1]


def summarize_pipeline_prediction(session_id: str) -> Optional[Dict[str, object]]:
    session_df = get_pipeline_session(session_id)
    if session_df.empty:
        return None

    latest_row = session_df.iloc[-1]
    feature_row = get_pipeline_feature_row(session_id)
    if feature_row is None:
        return {
            "session_id": session_id,
            "message": "æœ‰æ•ˆç‰¹å¾µä¸è¶³ï¼Œç„¡æ³•é€²è¡Œé æ¸¬",
            "latest": {
                "timestamp": str(latest_row["timestamp"]),
                "mvc_percent": float(latest_row["mvc_percent"]),
                "E_norm": float(latest_row["E_norm"]),
                "level": latest_row["level"],
                "trend": latest_row["trend"],
                "color": latest_row["color"],
                "blink": latest_row["blink"],
            },
        }

    X = feature_row[PIPELINE_FEATURE_COLUMNS].to_frame().T
    proba = predict_high_fatigue_probability(PIPELINE_ARTIFACTS.models, X)
    future_level = predict_future_level(PIPELINE_ARTIFACTS.models, X)

    return {
        "session_id": session_id,
        "latest": {
            "timestamp": str(latest_row["timestamp"]),
            "mvc_percent": float(latest_row["mvc_percent"]),
            "E_norm": float(latest_row["E_norm"]),
            "level": latest_row["level"],
            "trend": latest_row["trend"],
            "color": latest_row["color"],
            "blink": latest_row["blink"],
        },
        "predictions": {
            **proba,
            "future_level": future_level,
        },
    }


def compute_pipeline_prediction_table(session_id: str) -> pd.DataFrame:
    if PIPELINE_ARTIFACTS is None or PIPELINE_FEATURES_VALID.empty:
        return pd.DataFrame()

    session_feat = PIPELINE_FEATURES_VALID[
        PIPELINE_FEATURES_VALID["session_id"] == session_id
    ].sort_values("timestamp")
    if session_feat.empty:
        return pd.DataFrame()

    X = session_feat[PIPELINE_FEATURE_COLUMNS]
    out = session_feat[["timestamp"]].copy()

    if PIPELINE_ARTIFACTS.models.binary_logistic is not None:
        out["logistic_high_prob"] = PIPELINE_ARTIFACTS.models.binary_logistic.predict_proba(X)[:, 1]
    if PIPELINE_ARTIFACTS.models.binary_hgb is not None:
        out["hgb_high_prob"] = PIPELINE_ARTIFACTS.models.binary_hgb.predict_proba(X)[:, 1]
    if PIPELINE_ARTIFACTS.models.trinary_hgb is not None:
        out["future_level"] = PIPELINE_ARTIFACTS.models.trinary_hgb.predict(X)

    return out


def calculate_risk_level(mvc_change: float) -> tuple:
    """æ ¹æ“šMVCè®ŠåŒ–é‡è¨ˆç®—é¢¨éšªç­‰ç´š"""
    if mvc_change >= 30:
        return "é«˜åº¦", 3, "#e74c3c"
    elif mvc_change >= 15:
        return "ä¸­åº¦", 2, "#f39c12"
    else:
        return "ä½åº¦", 1, "#27ae60"

def predict_fatigue_risk(current_mvc: float, initial_mvc: float, time_elapsed: float) -> float:
    """ç°¡åŒ–ç‰ˆé æ¸¬ï¼šä»¥ç•¶å‰ MVC è®ŠåŒ–é‡èˆ‡æ™‚é–“æ¨ä¼°æœªä¾†é¢¨éšªã€‚"""
    mvc_change = current_mvc - initial_mvc
    # å‡è¨­æ¯åˆ†é˜ MVC æ¼¸é€²ä¸Šå‡ 0.05%ï¼Œä½œç‚ºä¿å®ˆé ä¼°ã€‚
    drift = 0.05 * time_elapsed
    return float(np.clip(mvc_change + drift, -20, 100))

# ==================== API ç«¯é» ====================

@app.get("/")
def home():
    return {
        "service": "ç–²å‹é æ¸¬ç³»çµ±",
        "version": "5.0",
        "description": "åŸºæ–¼ MVC/RULA èˆ‡ç–²å‹ç´¯ç©æ¨¡å‹çš„é æ¸¬æœå‹™",
        "endpoints": {
            "ä¸Šå‚³å–®ç­†": "POST /upload",
            "æ‰¹æ¬¡ä¸Šå‚³": "POST /upload_batch",
            "å³æ™‚ç‹€æ…‹": "GET /status/{worker_id}",
            "é æ¸¬æ•¸æ“š": "GET /predict/{worker_id}",
            "é æ¸¬åœ–è¡¨": "GET /chart/{worker_id}",
            "æ‰€æœ‰å·¥ä½œè€…": "GET /workers",
            "ç”Ÿæˆæ¸¬è©¦": "POST /test/generate/{worker_id}?count=10",
            "æ¸…ç©ºè³‡æ–™": "DELETE /clear/{worker_id}",
            "ç³»çµ±å¥åº·": "GET /health",
            "APIæ–‡ä»¶": "GET /docs",
            "æ¨¡æ“¬å ´æ™¯æ¸…å–®": "GET /dataset/sessions",
            "æ¨¡æ“¬ç‹€æ…‹": "GET /dataset/session/{session_id}",
            "æ¨¡æ“¬é æ¸¬": "GET /dataset/predict/{session_id}",
            "æ¨¡å‹è©•ä¼°": "GET /dataset/reports",
        }
    }


@app.get('/dataset/sessions')
def dataset_sessions():
    sessions = list_pipeline_sessions()
    return {"sessions": sessions, "count": len(sessions)}


@app.get('/dataset/session/{session_id}')
def dataset_session_detail(session_id: str, limit: int = 180):
    df = get_pipeline_session(session_id)
    if df.empty:
        raise HTTPException(404, f"æ‰¾ä¸åˆ°æ¨¡æ“¬å ´æ™¯ {session_id}")

    df_out = df if limit <= 0 else df.tail(limit)
    return {
        "session_id": session_id,
        "records": df_out.to_dict(orient='records'),
        "total": len(df),
    }


@app.get('/dataset/predict/{session_id}')
def dataset_prediction(session_id: str):
    summary = summarize_pipeline_prediction(session_id)
    if summary is None:
        raise HTTPException(404, f"æ‰¾ä¸åˆ°æ¨¡æ“¬å ´æ™¯ {session_id}")
    history_df = compute_pipeline_prediction_table(session_id)
    if not history_df.empty:
        summary["history"] = history_df.to_dict(orient='records')
    return summary


@app.get('/dataset/reports')
def dataset_reports():
    if not PIPELINE_REPORTS:
        raise HTTPException(503, "æ¨¡å‹å°šæœªæˆåŠŸè¨“ç·´")
    return {"reports": PIPELINE_REPORTS}

@app.post('/upload_rula')
def upload_rula(item: RulaData):
    """æ¥æ”¶å·²åœ¨ App ç«¯è¨ˆç®—å¥½çš„ RULA åˆ†æ•¸èˆ‡æ¨™ç±¤ï¼ˆä¸åšé‹ç®—ï¼‰ã€‚"""
    ts = item.timestamp or datetime.utcnow().isoformat()
    score = int(item.rula.get('score', 0))
    risk_label = item.rula.get('risk_label')
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute(
        "INSERT INTO rula_data (worker_id, timestamp, score, risk_label) VALUES (?, ?, ?, ?)",
        (item.worker_id, ts, score, str(risk_label) if risk_label is not None else None)
    )
    conn.commit()
    conn.close()
    print(f"[RULA] {item.worker_id} score={score} label={risk_label} ts={ts}")
    return {
        "status": "success",
        "worker_id": item.worker_id,
        "timestamp": ts,
        "rula": {"score": score, "risk_label": risk_label}
    }

# No server-side RULA API; the mobile app computes RULA before uploading.

@app.post('/upload')
def upload(item: SensorData):
    """ä¸Šå‚³å–®ç­†æ„Ÿæ¸¬å™¨è³‡æ–™ (ç”± APP è‡ªå‹•ä¸Šå‚³)"""
    ts = item.timestamp or datetime.utcnow().isoformat()
    
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute(
        "INSERT INTO sensor_data (worker_id, timestamp, percent_mvc) VALUES (?, ?, ?)",
        (item.worker_id, ts, item.percent_mvc)
    )
    conn.commit()
    conn.close()
    
    print(f"âœ… ä¸Šå‚³æˆåŠŸ: {item.worker_id} | MVC={item.percent_mvc}% | æ™‚é–“={ts}")
    
    return {
        "status": "success",
        "worker_id": item.worker_id,
        "timestamp": ts,
        "mvc": item.percent_mvc
    }

@app.post('/upload_batch')
def upload_batch(batch: BatchUpload):
    """æ‰¹æ¬¡ä¸Šå‚³å¤šç­†è³‡æ–™"""
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    
    for item in batch.data:
        ts = item.timestamp or datetime.utcnow().isoformat()
        c.execute(
            "INSERT INTO sensor_data (worker_id, timestamp, percent_mvc) VALUES (?, ?, ?)",
            (item.worker_id, ts, item.percent_mvc)
        )
    
    conn.commit()
    conn.close()
    
    print(f"âœ… æ‰¹æ¬¡ä¸Šå‚³æˆåŠŸ: {len(batch.data)} ç­†")
    
    return {
        "status": "success",
        "uploaded": len(batch.data)
    }

@app.get('/status/{worker_id}')
def get_status(worker_id: str):
    """å–å¾—å·¥ä½œè€…å³æ™‚ç‹€æ…‹"""

    summary = summarize_pipeline_prediction(worker_id)
    if summary is not None:
        session_df = get_pipeline_session(worker_id)
        latest = summary["latest"]
        return {
            "worker_id": worker_id,
            "source": "simulation",
            "timestamp": latest["timestamp"],
            "mvc_percent": latest["mvc_percent"],
            "E_norm": latest["E_norm"],
            "fatigue_level": latest["level"],
            "trend": latest["trend"],
            "led": {
                "color": latest["color"],
                "blink": latest["blink"],
            },
            "predictions": summary.get("predictions"),
            "message": summary.get("message"),
            "data_count": int(len(session_df)),
        }

    df = get_worker_data(worker_id)
    if df.empty:
        raise HTTPException(404, f"æ‰¾ä¸åˆ° {worker_id} çš„è³‡æ–™")

    latest = df.iloc[0]
    current_mvc = float(latest['percent_mvc'])
    initial = df.iloc[-1]
    initial_mvc = float(initial['percent_mvc'])
    mvc_change = current_mvc - initial_mvc
    time_diff = (latest['timestamp'] - initial['timestamp']).total_seconds() / 60
    risk_label, risk_level, risk_color = calculate_risk_level(mvc_change)
    recent_avg = df.head(10)['percent_mvc'].mean()

    return {
        "worker_id": worker_id,
        "current_mvc": round(current_mvc, 2),
        "initial_mvc": round(initial_mvc, 2),
        "mvc_change": round(mvc_change, 2),
        "mvc_change_rate": round(mvc_change / time_diff if time_diff > 0 else 0, 3),
        "fatigue_risk": risk_label,
        "risk_level": risk_level,
        "risk_color": risk_color,
        "time_elapsed_minutes": round(time_diff, 1),
        "recent_avg_mvc": round(recent_avg, 2),
        "last_update": str(latest['timestamp']),
        "data_count": len(df),
        "recommendation": get_recommendation(risk_level, mvc_change)
    }

def get_recommendation(risk_level: int, mvc_change: float) -> str:
    """æ ¹æ“šé¢¨éšªç­‰ç´šæä¾›å»ºè­°"""
    if risk_level == 3:
        return "âš ï¸ é«˜é¢¨éšªï¼å»ºè­°ç«‹å³ä¼‘æ¯ 15-20 åˆ†é˜"
    elif risk_level == 2:
        return "âš¡ ä¸­åº¦é¢¨éšªï¼Œå»ºè­°èª¿æ•´å·¥ä½œå§¿å‹¢æˆ–çŸ­æš«ä¼‘æ¯"
    else:
        return "âœ… ç‹€æ…‹è‰¯å¥½ï¼Œç¹¼çºŒä¿æŒ"

@app.get('/predict/{worker_id}')
def predict(worker_id: str, horizon: int = 120):
    """å–å¾—æœªä¾†ç–²å‹é¢¨éšªé æ¸¬ (JSON)"""

    summary = summarize_pipeline_prediction(worker_id)
    if summary is not None:
        history_df = compute_pipeline_prediction_table(worker_id)
        if not history_df.empty:
            summary["history"] = history_df.to_dict(orient='records')
        summary["horizon_minutes"] = int(HORIZON_S / 60)
        summary["note"] = "é æ¸¬ 30 åˆ†é˜å…§æ˜¯å¦æœƒé€²å…¥é«˜ç–²å‹"
        return summary

    df = get_worker_data(worker_id)
    if df.empty:
        raise HTTPException(404, f"æ‰¾ä¸åˆ° {worker_id} çš„è³‡æ–™")

    latest = df.iloc[0]
    initial = df.iloc[-1]
    current_mvc = float(latest['percent_mvc'])
    initial_mvc = float(initial['percent_mvc'])
    current_time = (latest['timestamp'] - initial['timestamp']).total_seconds() / 60

    predictions = []
    for t in range(0, min(horizon, 240) + 1, 10):
        future_time = current_time + t
        predicted_risk = predict_fatigue_risk(current_mvc, initial_mvc, future_time)
        predicted_mvc = initial_mvc + predicted_risk
        risk_label, _, _ = calculate_risk_level(predicted_risk)

        predictions.append({
            "minutes_from_now": t,
            "predicted_mvc": round(predicted_mvc, 2),
            "mvc_change": round(predicted_risk, 2),
            "risk_level": risk_label
        })

    return {
        "worker_id": worker_id,
        "current_state": {
            "mvc": current_mvc,
            "initial_mvc": initial_mvc,
            "current_change": round(current_mvc - initial_mvc, 2)
        },
        "predictions": predictions
    }

@app.get('/chart/{worker_id}')
def chart(worker_id: str, horizon: int = 120):
    """å–å¾—MVCè®ŠåŒ–é æ¸¬æ›²ç·šåœ–"""
    df = get_worker_data(worker_id)
    if df.empty:
        raise HTTPException(404, f"æ‰¾ä¸åˆ° {worker_id} çš„è³‡æ–™")
    
    latest = df.iloc[0]
    initial = df.iloc[-1]
    
    current_mvc = float(latest['percent_mvc'])
    initial_mvc = float(initial['percent_mvc'])
    current_time = (latest['timestamp'] - initial['timestamp']).total_seconds() / 60
    
    # ç”Ÿæˆé æ¸¬
    times = np.arange(0, min(horizon, 240) + 1, 5)
    mvc_values = []
    
    for t in times:
        future_time = current_time + t
        predicted_risk = predict_fatigue_risk(current_mvc, initial_mvc, future_time)
        predicted_mvc = initial_mvc + predicted_risk
        mvc_values.append(predicted_mvc)
    
    # ç¹ªåœ–
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
    
    # åœ–1: MVCçµ•å°å€¼é æ¸¬
    ax1.plot(times, mvc_values, 'o-', color='#667eea', linewidth=2.5, markersize=5)
    ax1.axhline(initial_mvc, color='green', linestyle='--', linewidth=1.5, alpha=0.7, label='åˆå§‹ MVC')
    ax1.axhline(current_mvc, color='blue', linestyle='--', linewidth=1.5, alpha=0.7, label='ç•¶å‰ MVC')
    ax1.fill_between(times, initial_mvc + 10, initial_mvc + 20, alpha=0.2, color='orange', label='ä¸­åº¦é¢¨éšªå€')
    ax1.fill_between(times, initial_mvc + 20, 100, alpha=0.2, color='red', label='é«˜åº¦é¢¨éšªå€')
    ax1.set_xlabel('æœªä¾†åˆ†é˜æ•¸', fontsize=12, fontweight='bold')
    ax1.set_ylabel('é æ¸¬ MVC (%)', fontsize=12, fontweight='bold')
    ax1.set_title(f'MVC ç™¾åˆ†æ¯”é æ¸¬ - {worker_id}', fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    ax1.legend(loc='best')
    ax1.set_ylim([0, 100])
    
    # åœ–2: MVCè®ŠåŒ–é‡
    mvc_changes = np.array(mvc_values) - initial_mvc
    ax2.plot(times, mvc_changes, 'o-', color='#f5576c', linewidth=2.5, markersize=5)
    ax2.axhline(0, color='gray', linestyle='-', linewidth=1, alpha=0.5)
    ax2.axhline(10, color='orange', linestyle='--', linewidth=1.5, alpha=0.7, label='ä¸­åº¦é¢¨éšª (+10%)')
    ax2.axhline(20, color='red', linestyle='--', linewidth=1.5, alpha=0.7, label='é«˜åº¦é¢¨éšª (+20%)')
    ax2.fill_between(times, 10, 20, alpha=0.2, color='orange')
    ax2.fill_between(times, 20, 100, alpha=0.2, color='red')
    ax2.set_xlabel('æœªä¾†åˆ†é˜æ•¸', fontsize=12, fontweight='bold')
    ax2.set_ylabel('MVC è®ŠåŒ–é‡ (%)', fontsize=12, fontweight='bold')
    ax2.set_title(f'MVC è®ŠåŒ–é‡é æ¸¬ - {worker_id}', fontsize=14, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    ax2.legend(loc='best')
    
    buf = io.BytesIO()
    plt.tight_layout()
    plt.savefig(buf, format='png', dpi=120)
    plt.close()
    buf.seek(0)
    
    return Response(content=buf.getvalue(), media_type='image/png')

@app.get('/workers')
def list_workers():
    """åˆ—å‡ºæ‰€æœ‰å·¥ä½œè€…"""
    conn = sqlite3.connect(DB_PATH)
    df = pd.read_sql_query(
        """SELECT worker_id, COUNT(*) as count, 
           MAX(timestamp) as last_update,
           AVG(percent_mvc) as avg_mvc,
           MIN(percent_mvc) as min_mvc,
           MAX(percent_mvc) as max_mvc
           FROM sensor_data 
           GROUP BY worker_id""",
        conn
    )
    conn.close()
    
    # è¨ˆç®—æ¯å€‹å·¥ä½œè€…çš„é¢¨éšªç‹€æ…‹
    workers = []
    for _, row in df.iterrows():
        mvc_range = row['max_mvc'] - row['min_mvc']
        risk_label, risk_level, risk_color = calculate_risk_level(mvc_range)
        
        workers.append({
            "worker_id": row['worker_id'],
            "count": int(row['count']),
            "last_update": row['last_update'],
            "avg_mvc": round(row['avg_mvc'], 2),
            "min_mvc": round(row['min_mvc'], 2),
            "max_mvc": round(row['max_mvc'], 2),
            "mvc_range": round(mvc_range, 2),
            "risk_level": risk_label,
            "risk_color": risk_color
        })
    
    return {"workers": workers, "total": len(workers)}

@app.delete('/clear/{worker_id}')
def clear(worker_id: str):
    """æ¸…ç©ºå·¥ä½œè€…è³‡æ–™"""
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("DELETE FROM sensor_data WHERE worker_id = ?", (worker_id,))
    deleted = c.rowcount
    conn.commit()
    conn.close()
    
    print(f"ğŸ—‘ï¸ å·²åˆªé™¤ {worker_id} çš„ {deleted} ç­†è³‡æ–™")
    
    return {
        "status": "success",
        "worker_id": worker_id,
        "deleted": deleted,
        "message": f"å·²æˆåŠŸåˆªé™¤ {worker_id} çš„æ‰€æœ‰è¨˜éŒ„"
    }

@app.delete('/clear_all')
def clear_all():
    """æ¸…ç©ºæ‰€æœ‰è³‡æ–™ï¼ˆæ…ç”¨ï¼‰"""
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("SELECT COUNT(*) FROM sensor_data")
    total = c.fetchone()[0]
    c.execute("DELETE FROM sensor_data")
    conn.commit()
    conn.close()
    
    print(f"ğŸ—‘ï¸ å·²æ¸…ç©ºè³‡æ–™åº«ï¼Œåˆªé™¤ {total} ç­†è³‡æ–™")
    
    return {
        "status": "success",
        "deleted": total,
        "message": f"å·²æ¸…ç©ºæ‰€æœ‰è³‡æ–™ï¼Œå…±åˆªé™¤ {total} ç­†è¨˜éŒ„"
    }

@app.post('/test/generate/{worker_id}')
def generate_test(worker_id: str, count: int = 20):
    """ç”Ÿæˆæ¸¬è©¦è³‡æ–™ (æ¨¡æ“¬æ¯åˆ†é˜ä¸Šå‚³)"""
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    
    # å¾30%é–‹å§‹ï¼Œéš¨æ©Ÿéå¢æ¨¡æ“¬ç–²å‹ç´¯ç©
    base_mvc = 30.0
    
    for i in range(count):
        ts = (datetime.utcnow() - timedelta(minutes=count-i)).isoformat()
        # MVCéš¨æ™‚é–“å¢åŠ ï¼Œæ¨¡æ“¬ç–²å‹ç´¯ç©
        mvc = base_mvc + i * np.random.uniform(0.5, 2.0) + np.random.normal(0, 2)
        mvc = np.clip(mvc, 0, 100)
        
        c.execute(
            "INSERT INTO sensor_data (worker_id, timestamp, percent_mvc) VALUES (?, ?, ?)",
            (worker_id, ts, float(mvc))
        )
    
    conn.commit()
    conn.close()
    
    print(f"âœ… å·²ç”Ÿæˆ {count} ç­†æ¸¬è©¦è³‡æ–™ (æ¨¡æ“¬æ¯åˆ†é˜ä¸Šå‚³)")
    
    return {
        "status": "success",
        "worker_id": worker_id,
        "generated": count,
        "message": f"å·²ç”Ÿæˆ {count} ç­†æ¸¬è©¦è³‡æ–™ï¼Œæ¨¡æ“¬ {count} åˆ†é˜çš„å·¥ä½œè¨˜éŒ„"
    }

@app.get('/health')
def health():
    """ç³»çµ±å¥åº·æª¢æŸ¥"""
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("SELECT COUNT(*) FROM sensor_data")
    total = c.fetchone()[0]
    c.execute("SELECT COUNT(DISTINCT worker_id) FROM sensor_data")
    workers = c.fetchone()[0]
    conn.close()
    
    return {
        "status": "healthy",
        "pipeline_loaded": PIPELINE_ARTIFACTS is not None,
        "simulated_sessions": len(list_pipeline_sessions()),
        "model_reports": PIPELINE_REPORTS,
        "total_records": total,
        "total_workers": workers,
        "database": DB_PATH,
        "version": "5.0",
    }

if __name__ == '__main__':
    print("ğŸš€ ç–²å‹é æ¸¬ç³»çµ±å·²å•Ÿå‹• (v5.0)")
    print("ğŸ“ æœ¬æ©Ÿ: http://localhost:8000")
    print("ğŸ“ æ–‡ä»¶: http://localhost:8000/docs")
    print("ğŸ’¡ ç³»çµ±ç‰¹é»: æ•´åˆ 30 ç§’æ¡æ¨£ç–²å‹ç´¯ç©æ¨¡å‹ + Logistic/HGB é æ¸¬")
    print("â±ï¸  å»ºè­° APP æ¯ 30 ç§’æˆ– 1 åˆ†é˜è‡ªå‹•ä¸Šå‚³è³‡æ–™")
    print("\nå•Ÿå‹•å‘½ä»¤: uvicorn fastapi_fatigue_service:app --reload --host 0.0.0.0 --port 8000")
